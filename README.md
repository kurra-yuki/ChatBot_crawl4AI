# ğŸ§  ãƒ©ã‚·ã‚­ã‚¢ã‚¼ãƒŸå‘ã‘ AIãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆé–‹ç™ºãƒ¬ã‚¯ãƒãƒ£ãƒ¼

---

## ğŸ¯ ã‚´ãƒ¼ãƒ«

ãƒ©ã‚·ã‚­ã‚¢ã‚¼ãƒŸã®Webã‚µã‚¤ãƒˆï¼ˆhttps://lanet.sist.chukyo-u.ac.jp/ï¼‰ã®æƒ…å ±ã‚’ã‚‚ã¨ã«ã€  
è³ªå•ã«è‡ªå‹•ã§ç­”ãˆã‚‹**AIãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ**ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚

---

## ğŸ›  ä½¿ç”¨æŠ€è¡“ä¸€è¦§

| æŠ€è¡“         | å†…å®¹ |
|--------------|------|
| Crawl4AI     | Webã‚µã‚¤ãƒˆã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º |
| Chroma       | ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«å½¢å¼ã§ä¿å­˜ãƒ»æ¤œç´¢ |
| LangChain    | ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãƒ»RAGæ§‹æˆã®æ”¯æ´ |
| OpenAI API   | ChatGPTãƒ¢ãƒ‡ãƒ«ã§å›ç­”ç”Ÿæˆ |
| Streamlit    | ç°¡å˜ãªWebãƒãƒ£ãƒƒãƒˆUIã‚’ä½œæˆ |

---

## ğŸ”§ æº–å‚™

### 1. Pythonç’°å¢ƒã‚’æ•´ãˆã‚‹

Python 3.9 ä»¥ä¸Šï¼ˆ3.10ã‚‚å¯ï¼‰

```bash
python -m venv venv
venv\Scripts\activate  # Mac ã®å ´åˆ: source venv/bin/activate
```

### 2. å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

requirements.txtã‹ã‚‰ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```bash
crawl4ai
langchain
chromadb
openai
tiktoken
streamlit
python-dotenv
```

<br>


```bash
pip install -r requirements.txt
```

### 3. OpenAI APIã‚­ãƒ¼ã®æº–å‚™
.envãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
```ini
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxx
```

---

## ğŸ—ºï¸ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ§‹æˆå›³ï¼ˆRAGï¼‰

```mermaid
graph TD
  A[LANet Webã‚µã‚¤ãƒˆ] -->|Crawl4AI| B[ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ç¾¤]
  B -->|åˆ†å‰²ï¼†ãƒ™ã‚¯ãƒˆãƒ«åŒ–| C[Chromaãƒ™ã‚¯ãƒˆãƒ«DB]
  D[ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•] --> E[ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢]
  C --> E
  E --> F[ChatGPTï¼ˆç”Ÿæˆï¼‰]
  F --> G[Streamlitãƒãƒ£ãƒƒãƒˆç”»é¢]
```
---
<div style="page-break-after: always;"></div>


## âœ… ã‚¹ãƒ†ãƒƒãƒ—1ï¼šã‚µã‚¤ãƒˆã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ï¼ˆãƒ‡ãƒ¼ã‚¿åé›†ï¼‰
ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€crawl4ai ã‚’ç”¨ã„ã¦ ãƒ©ã‚·ã‚­ã‚¢ã‚¼ãƒŸWebã‚µã‚¤ãƒˆã®è¤‡æ•°ãƒšãƒ¼ã‚¸ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã€Markdownå½¢å¼ã®æœ¬æ–‡ã‚’ .txt ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚CLIã¯ä½¿ç”¨ã›ãšã€Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã‹ã‚‰ç›´æ¥ AsyncWebCrawler ã‚’åˆ¶å¾¡ã—ã¾ã™ã€‚
```python
# crawl_lanet_session_save.py
import asyncio
from typing import List
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
import os
import re

# ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾è±¡URLï¼ˆå¿…è¦ã«å¿œã˜ã¦è¿½åŠ ï¼‰
TARGET_URLS = [
    "https://lanet.sist.chukyo-u.ac.jp/",
    "https://lanet.sist.chukyo-u.ac.jp/activities",
    "https://lanet.sist.chukyo-u.ac.jp/societies",
    "https://lanet.sist.chukyo-u.ac.jp/researches",
    "https://lanet.sist.chukyo-u.ac.jp/jobs",
    "https://lanet.sist.chukyo-u.ac.jp/members",
    "https://lanet.sist.chukyo-u.ac.jp/links"
]

# ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
OUTPUT_DIR = "lanet_data"
os.makedirs(OUTPUT_DIR, exist_ok=True)

def sanitize_filename(url: str) -> str:
    return re.sub(r'[^\w\-_.]', '_', url.strip("/"))[:100]

async def crawl_sequential_and_save(urls: List[str]):
    print("\n=== Crawl4AI + ã‚»ãƒƒã‚·ãƒ§ãƒ³å†åˆ©ç”¨ + Markdownä¿å­˜ ===")

    browser_config = BrowserConfig(
        headless=True,
        extra_args=["--disable-gpu", "--disable-dev-shm-usage", "--no-sandbox"],
    )

    crawl_config = CrawlerRunConfig(
        markdown_generator=DefaultMarkdownGenerator()
    )

    crawler = AsyncWebCrawler(config=browser_config)
    await crawler.start()

    try:
        session_id = "lanet_session"
        for url in urls:
            result = await crawler.arun(
                url=url,
                config=crawl_config,
                session_id=session_id
            )

            if result.success:
                print(f"âœ… Success: {url}")
                filename = sanitize_filename(url)
                path = os.path.join(OUTPUT_DIR, f"{filename}.txt")
                with open(path, "w", encoding="utf-8") as f:
                    f.write(result.markdown.raw_markdown or "")
                print(f"ğŸ“„ ä¿å­˜: {path}")
            else:
                print(f"âŒ Failed: {url} - {result.error_message}")
    finally:
        await crawler.close()
        print("âœ… ã‚¯ãƒ­ãƒ¼ãƒ«å®Œäº†ï¼ˆã™ã¹ã¦ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é–‰ã˜ã¾ã—ãŸï¼‰")

async def main():
    await crawl_sequential_and_save(TARGET_URLS)

if __name__ == "__main__":
    asyncio.run(main())
```
ãƒ»çµæœï¼šlanet_data/ ãƒ•ã‚©ãƒ«ãƒ€ã« .txt ãƒ•ã‚¡ã‚¤ãƒ«ç¾¤ãŒä¿å­˜ã•ã‚Œã¾ã™ã€‚

âœ… Markdownï¼ˆMDå½¢å¼ï¼‰ã«ã™ã‚‹ä¸»ãªç†ç”±

| å¯¾è±¡     | MDå½¢å¼ã§ã®åˆ©ç‚¹                  |
| ------ | ------------------------- |
| æ¤œç´¢ç²¾åº¦   | è¦‹å‡ºã—å˜ä½ãƒ»æ®µè½å˜ä½ã®é–¢é€£æ€§ãŒæ˜ç¢ºã«ãªã‚‹      |
| LLMå›ç­”  | ã€Œã“ã®ãƒ†ãƒ¼ãƒã«ã¤ã„ã¦ã€ã®å•ã„ã«ä¸€è²«ã—ãŸæ–‡è„ˆã§è¿”ã›ã‚‹ |
| å°†æ¥ã®å†åˆ©ç”¨ | PDFãƒ»HTMLåŒ–ãƒ»è¡¨ç¤ºã«ã‚‚å¿œç”¨ã—ã‚„ã™ã„      |

âœ… ä»–ã¨ã®æ¯”è¼ƒã¾ã¨ã‚ï¼ˆè¡¨ï¼‰
| ãƒ„ãƒ¼ãƒ«                  | ç‰¹å¾´                               | å‘ã„ã¦ã„ã‚‹ç”¨é€”                  |
| -------------------- | -------------------------------- | ------------------------ |
| **Crawl4AI**         | Markdownå‡ºåŠ›ã€éåŒæœŸã€Playwrightã€RAGæœ€é©åŒ– | AIãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã€RAGå‰å‡¦ç†ã€LLMå­¦ç¿’ç´ æ |
| **BeautifulSoup**    | è»½é‡ã€ã‚·ãƒ³ãƒ—ãƒ«ã€ã‚«ã‚¹ã‚¿ãƒ è§£æå®¹æ˜“                 | å°è¦æ¨¡ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã€ç‰¹å®šè¦ç´ ã®æŠ½å‡º       |
| **Scrapy**           | é«˜é€Ÿãƒ»æ‹¡å¼µæ€§ãƒ»ã‚¯ãƒ­ãƒ¼ãƒ©ç®¡ç†                    | å¤§è¦æ¨¡Webã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã€åˆ¶å¾¡ãŒå¿…è¦ãªå ´åˆ    |
| **Selenium**         | JSãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã€æ“ä½œå†ç¾æ€§ã‚ã‚Š                 | è‡ªå‹•åŒ–ãƒ»ãƒ–ãƒ©ã‚¦ã‚¶æ“ä½œãŒå¿…è¦ãªæ¤œè¨¼ç³»        |
| **readability-lxml** | ä¸»è¦æœ¬æ–‡æŠ½å‡ºã«ç‰¹åŒ–ï¼ˆç°¡å˜ï¼‰                    | ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚„ãƒ–ãƒ­ã‚°ãªã©èª­ã¿ã‚„ã™ã•é‡è¦–ã®æŠ½å‡º     |


## âœ… ã‚¹ãƒ†ãƒƒãƒ—2ï¼šãƒ™ã‚¯ãƒˆãƒ«DBã‚’æ§‹ç¯‰ï¼ˆLangChain + Chromaï¼‰
build_vector_db.py
```python
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from dotenv import load_dotenv
import os

# .env ã‹ã‚‰ OPENAI_API_KEY ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

# Markdownãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
loader = DirectoryLoader(
    "lanet_data",
    glob="**/*.txt",
    loader_cls=lambda path: TextLoader(path, encoding="utf-8")
)
docs = loader.load()
print(f"ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(docs)}")

# ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
split_docs = splitter.split_documents(docs)
print(f"âœ‚ï¸ åˆ†å‰²å¾Œãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(split_docs)}")

# OpenAI åŸ‹ã‚è¾¼ã¿ + Chromaã¸ä¿å­˜
embedding = OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))
db = Chroma.from_documents(split_docs, embedding, persist_directory="lanet_chroma_md")

print("âœ… ãƒ™ã‚¯ãƒˆãƒ«DBä½œæˆå®Œäº†ï¼ˆMarkdownå¯¾å¿œï¼‰")
```
å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰
```bash
python build_vector_db.py
```

## âœ… ã‚¹ãƒ†ãƒƒãƒ—3ï¼šãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚¢ãƒ—ãƒªã‚’æ§‹ç¯‰ï¼ˆStreamlitï¼‰
app.py
```python
import streamlit as st
from dotenv import load_dotenv
import os

from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain.chains import RetrievalQA

# .envã‹ã‚‰APIã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

st.title("ãƒ©ã‚·ã‚­ã‚¢ç ”ç©¶å®¤ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ ğŸ¤–")
query = st.text_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

# ãƒ™ã‚¯ãƒˆãƒ«DBãƒ­ãƒ¼ãƒ‰
embedding = OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))
db = Chroma(persist_directory="lanet_chroma_md", embedding_function=embedding)

# ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã¨RAGãƒã‚§ãƒ¼ãƒ³ã®è¨­å®š
llm = ChatOpenAI(temperature=0, model_name="gpt-3.5-turbo", openai_api_key=os.getenv("OPENAI_API_KEY"))
qa = RetrievalQA.from_chain_type(llm=llm, retriever=db.as_retriever())

# å¿œç­”å‡¦ç†
if query:
    with st.spinner("è€ƒãˆä¸­..."):
        result = qa.invoke({"query": query})
        st.success(result["result"])
```

## âœ… ã‚¹ãƒ†ãƒƒãƒ—4ï¼šStreamlitã‚¢ãƒ—ãƒªã‚’èµ·å‹•
```bash
streamlit run app.py
```

## âœ… å‹•ä½œä¾‹

è³ªå•ä¾‹ï¼š<br>
ãƒ»ã€ŒLANetã‚¼ãƒŸã®ç ”ç©¶ãƒ†ãƒ¼ãƒã¯ä½•ã§ã™ã‹ï¼Ÿã€<br>
ãƒ»ã€Œã‚¼ãƒŸã§ã¯ã©ã‚“ãªæ´»å‹•ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿã€<br>
ãƒ»ã€Œå’æ¥­ç ”ç©¶ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€<br>
"# ChatBot_crawl4AI" 
