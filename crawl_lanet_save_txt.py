import asyncio, os, re
from typing import List
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

# ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾è±¡URLï¼ˆå¿…è¦ã«å¿œã˜ã¦è¿½åŠ ï¼‰
TARGET_URLS = [
    "https://lanet.sist.chukyo-u.ac.jp/",
    "https://lanet.sist.chukyo-u.ac.jp/activities",
    "https://lanet.sist.chukyo-u.ac.jp/societies",
    "https://lanet.sist.chukyo-u.ac.jp/researches",
    "https://lanet.sist.chukyo-u.ac.jp/jobs",
    "https://lanet.sist.chukyo-u.ac.jp/members",
    "https://lanet.sist.chukyo-u.ac.jp/links"
]

# ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
OUTPUT_DIR = "lanet_data"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ãƒ•ã‚¡ã‚¤ãƒ«åã®ã‚µãƒ‹ã‚¿ã‚¤ã‚ºé–¢æ•°
def sanitize_filename(url: str) -> str:
    return re.sub(r'[^\w\-_.]', '_', url.strip("/"))[:100]

# ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã¨Markdownå½¢å¼ã§ä¿å­˜ã™ã‚‹é–¢æ•°
async def crawl_sequential_and_save(urls: List[str]):
    print("\n=== MarkDownå½¢å¼ã§ä¿å­˜ä¸­ ===")

    # ãƒ–ãƒ©ã‚¦ã‚¶è¨­å®š
    browser_config = BrowserConfig(
        headless=True,
        extra_args=["--disable-gpu", "--disable-dev-shm-usage", "--no-sandbox"],
    )

    # ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°è¨­å®š
    crawl_config = CrawlerRunConfig(
        markdown_generator=DefaultMarkdownGenerator()
    )

    # éåŒæœŸã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã®åˆæœŸåŒ–
    crawler = AsyncWebCrawler(config=browser_config)
    await crawler.start()

    try:
        session_id = "lanet_session"
        for url in urls:
            # ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°
            result = await crawler.arun(
                url=url,
                config=crawl_config,
                session_id=session_id
            )

            # çµæœã®æ›¸ãè¾¼ã¿
            if result.success:
                print(f"âœ… Success: {url}")
                filename = sanitize_filename(url)
                path = os.path.join(OUTPUT_DIR, f"{filename}.txt")
                with open(path, "w", encoding="utf-8") as f:
                    f.write(result.markdown.raw_markdown or "")
                print(f"ğŸ“„ ä¿å­˜: {path}")
            else:
                print(f"âŒ Failed: {url} - {result.error_message}")
    finally:
        await crawler.close()
        print("âœ… ã‚¯ãƒ­ãƒ¼ãƒ«å®Œäº†ï¼ˆã™ã¹ã¦ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é–‰ã˜ã¾ã—ãŸï¼‰")

async def main():
    await crawl_sequential_and_save(TARGET_URLS)

if __name__ == "__main__":
    asyncio.run(main())
